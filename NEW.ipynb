{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1813787a",
   "metadata": {},
   "source": [
    "## Training the dataset for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7be140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the JSON dataset\n",
    "with open('primate_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare Data\n",
    "X = [entry['post_text'] for entry in data]\n",
    "\n",
    "annotations = [[annotation[0] for annotation in entry['annotations']] for entry in data]\n",
    "answers = [[annotation[1] for annotation in entry['annotations']] for entry in data]\n",
    "\n",
    "# Vectorize Text Data\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, answers, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536260de",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "## Model Used:- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba805c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.9998612845054793\n",
      "Precision: 0.999861392026659\n",
      "Recall: 0.9998612845054793\n",
      "F1 Score: 0.9998611583320177\n",
      "AUC-ROC Score: 0.9999996409009927\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.7788861180382377\n",
      "Precision: 0.7737952246224777\n",
      "Recall: 0.7788861180382377\n",
      "F1 Score: 0.7758290925675633\n",
      "AUC-ROC Score: 0.6664565569747141\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "train_auc_roc_scores = []\n",
    "test_auc_roc_scores = []\n",
    "\n",
    "for i in range(len(annotations[0])):\n",
    "    model = DecisionTreeClassifier()\n",
    "    y_train_question = [answer[i] for answer in y_train]\n",
    "    \n",
    "    model.fit(X_train, y_train_question)\n",
    "    models.append(model)\n",
    "\n",
    "    # Predict probabilities for the training set\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]  \n",
    "    train_auc_roc = roc_auc_score(y_train_question, y_train_prob)\n",
    "    \n",
    "    # Predict probabilities for the test set\n",
    "    y_test_question = [answer[i] for answer in y_test]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1] \n",
    "    test_auc_roc = roc_auc_score(y_test_question, y_test_prob)\n",
    "    \n",
    "    train_auc_roc_scores.append(train_auc_roc)\n",
    "    test_auc_roc_scores.append(test_auc_roc)\n",
    "\n",
    "# Step 6: Evaluate the Models\n",
    "#for accuracy\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "#for precision\n",
    "train_precisions = []\n",
    "test_precisions = []\n",
    "#for recalls\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "#for f1_score\n",
    "train_f1_scores = []\n",
    "test_f1_scores = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_train_question = [answer[i] for answer in y_train]\n",
    "    y_test_question = [answer[i] for answer in y_test]\n",
    "    \n",
    "    # Training metrics\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train_question, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    train_precision = precision_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_precisions.append(train_precision)\n",
    "\n",
    "    train_recall = recall_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_recalls.append(train_recall)\n",
    "\n",
    "    train_f1 = f1_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Test metrics\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test_question, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    test_precision = precision_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_precisions.append(test_precision)\n",
    "\n",
    "    test_recall = recall_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_recalls.append(test_recall)\n",
    "\n",
    "    test_f1 = f1_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_f1_scores.append(test_f1)\n",
    "\n",
    "# Calculate averages\n",
    "avg_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1_score = sum(train_f1_scores) / len(train_f1_scores)\n",
    "avg_train_auc_roc_score = sum(train_auc_roc_scores) / len(train_auc_roc_scores)\n",
    "\n",
    "avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "avg_test_precision = sum(test_precisions) / len(test_precisions)\n",
    "avg_test_recall = sum(test_recalls) / len(test_recalls)\n",
    "avg_test_f1_score = sum(test_f1_scores) / len(test_f1_scores)\n",
    "avg_test_auc_roc_score = sum(test_auc_roc_scores) / len(test_auc_roc_scores)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {avg_train_accuracy}\")\n",
    "print(f\"Precision: {avg_train_precision}\")\n",
    "print(f\"Recall: {avg_train_recall}\")\n",
    "print(f\"F1 Score: {avg_train_f1_score}\")\n",
    "print(f\"AUC-ROC Score: {avg_train_auc_roc_score}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {avg_test_accuracy}\")\n",
    "print(f\"Precision: {avg_test_precision}\")\n",
    "print(f\"Recall: {avg_test_recall}\")\n",
    "print(f\"F1 Score: {avg_test_f1_score}\")\n",
    "print(f\"AUC-ROC Score: {avg_test_auc_roc_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab234d3c",
   "metadata": {},
   "source": [
    "## Model Used :- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e394be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 0.9998612845054793\n",
      "Precision: 0.999861392026659\n",
      "Recall: 0.9998612845054793\n",
      "F1 Score: 0.9998611583320177\n",
      "AUC-ROC Score: 0.9999996409009927\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.7763923524522028\n",
      "Precision: 0.7332666989259001\n",
      "Recall: 0.7763923524522028\n",
      "F1 Score: 0.7229965944651027\n",
      "AUC-ROC Score: 0.7663612622893038\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Step 1: Load the JSON dataset\n",
    "with open('primate_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 2: Prepare Data\n",
    "X = [entry['post_text'] for entry in data]\n",
    "\n",
    "annotations = [[annotation[0] for annotation in entry['annotations']] for entry in data]\n",
    "answers = [[annotation[1] for annotation in entry['annotations']] for entry in data]\n",
    "\n",
    "# Step 3: Vectorize Text Data\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, answers, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Random Forest Models\n",
    "models = []\n",
    "train_auc_roc_scores = []\n",
    "test_auc_roc_scores = []\n",
    "\n",
    "for i in range(len(annotations[0])):\n",
    "    model = RandomForestClassifier()  # Change to RandomForestClassifier\n",
    "    y_train_question = [answer[i] for answer in y_train]\n",
    "    \n",
    "    model.fit(X_train, y_train_question)\n",
    "    models.append(model)\n",
    "\n",
    "    # Predict probabilities for the training set\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]  \n",
    "    train_auc_roc = roc_auc_score(y_train_question, y_train_prob)\n",
    "    \n",
    "    # Predict probabilities for the test set\n",
    "    y_test_question = [answer[i] for answer in y_test]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1] \n",
    "    test_auc_roc = roc_auc_score(y_test_question, y_test_prob)\n",
    "    \n",
    "    train_auc_roc_scores.append(train_auc_roc)\n",
    "    test_auc_roc_scores.append(test_auc_roc)\n",
    "\n",
    "# Step 6: Evaluate the Models\n",
    "# for accuracy\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "# for precision\n",
    "train_precisions = []\n",
    "test_precisions = []\n",
    "# for recalls\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "# for f1_score\n",
    "train_f1_scores = []\n",
    "test_f1_scores = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_train_question = [answer[i] for answer in y_train]\n",
    "    y_test_question = [answer[i] for answer in y_test]\n",
    "    \n",
    "    # Training metrics\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train_question, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    train_precision = precision_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_precisions.append(train_precision)\n",
    "\n",
    "    train_recall = recall_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_recalls.append(train_recall)\n",
    "\n",
    "    train_f1 = f1_score(y_train_question, y_train_pred, average='weighted')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Test metrics\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test_question, y_test_pred)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    test_precision = precision_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_precisions.append(test_precision)\n",
    "\n",
    "    test_recall = recall_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_recalls.append(test_recall)\n",
    "\n",
    "    test_f1 = f1_score(y_test_question, y_test_pred, average='weighted')\n",
    "    test_f1_scores.append(test_f1)\n",
    "\n",
    "# Step 7: Calculate averages\n",
    "avg_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1_score = sum(train_f1_scores) / len(train_f1_scores)\n",
    "avg_train_auc_roc_score = sum(train_auc_roc_scores) / len(train_auc_roc_scores)\n",
    "\n",
    "avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "avg_test_precision = sum(test_precisions) / len(test_precisions)\n",
    "avg_test_recall = sum(test_recalls) / len(test_recalls)\n",
    "avg_test_f1_score = sum(test_f1_scores) / len(test_f1_scores)\n",
    "avg_test_auc_roc_score = sum(test_auc_roc_scores) / len(test_auc_roc_scores)\n",
    "\n",
    "# Step 8: Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {avg_train_accuracy}\")\n",
    "print(f\"Precision: {avg_train_precision}\")\n",
    "print(f\"Recall: {avg_train_recall}\")\n",
    "print(f\"F1 Score: {avg_train_f1_score}\")\n",
    "print(f\"AUC-ROC Score: {avg_train_auc_roc_score}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {avg_test_accuracy}\")\n",
    "print(f\"Precision: {avg_test_precision}\")\n",
    "print(f\"Recall: {avg_test_recall}\")\n",
    "print(f\"F1 Score: {avg_test_f1_score}\")\n",
    "print(f\"AUC-ROC Score: {avg_test_auc_roc_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e318c",
   "metadata": {},
   "source": [
    "## Testing the prediction ability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d9baa",
   "metadata": {},
   "source": [
    "## file input and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0c1af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path of the input .txt file: tr1.txt\n",
      "enter the file name:o\n",
      "Content successfully written to o\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = input(\"Enter the path of the input .txt file: \")\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        text_content = file.read()\n",
    "        text_content = text_content[1:]\n",
    "        text_content = text_content[:-1]\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n",
    "\n",
    "    \n",
    "# Load the new data (example data)\n",
    "new_data = [\n",
    "    {       \"post_text\":text_content}]\n",
    "# Preprocess the new data\n",
    "new_X = [entry['post_text'] for entry in new_data]\n",
    "new_X_vectorized = vectorizer.transform(new_X)\n",
    "\n",
    "# Use the trained models to predict annotations\n",
    "new_annotations = []\n",
    "for model in models:\n",
    "    new_predictions = model.predict(new_X_vectorized)\n",
    "    new_annotations.append(new_predictions)\n",
    "\n",
    "# Aggregate the predictions into the desired format\n",
    "last=[]\n",
    "for i, entry in enumerate(new_data):\n",
    "    for j, question in enumerate(annotations[0]):\n",
    "        b=[]\n",
    "        b.append(question)\n",
    "        b.append(new_annotations[j][i])\n",
    "        last.append(b)\n",
    "\n",
    "con=[str(x) for x in last]\n",
    "content=','.join(con)\n",
    "file_path1=input('enter the file name:')\n",
    "try:\n",
    "    with open(file_path1, 'w') as file:\n",
    "        file.write('{'+content+'}')\n",
    "    print(f\"Content successfully written to {file_path1}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc0139",
   "metadata": {},
   "source": [
    " ## GUI Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d64dfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "\n",
    "class TextColors:\n",
    "    RESET = '\\033[0m'\n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "\n",
    "# Assuming you have the rest of your script before this point...\n",
    "n=0\n",
    "t=0\n",
    "nt=0\n",
    "# Function to update the conclusion label based on conditions\n",
    "def update_conclusion():\n",
    "    global n, t, nt\n",
    "    y = [entry[1] for entry in last]\n",
    "\n",
    "    if 'yes' in (y[6], y[7]):\n",
    "        conclusion_label.config(text=\"CONCLUSION: CATASTROPHIC\", foreground=\"red\")\n",
    "        messagebox.showinfo(\"Treatment\", \"Ambulance Number: XYZ\\nHelpline Number: ABC/WXY/CED\")\n",
    "    else:\n",
    "        c = y.count('yes')\n",
    "        if c <= 3:\n",
    "            conclusion_label.config(text=\"CONCLUSION: HEALTHY\", foreground=\"green\")\n",
    "            n += 1\n",
    "        else:\n",
    "            conclusion_label.config(text=\"CONCLUSION: VULNERABLE\", foreground=\"orange\")\n",
    "            messagebox.showinfo(\"Treatment\", \"Make your bedroom sleep-friendly\\nGo to sleep and wake up around the same time each day, even on the weekends\\nAvoid caffeine, nicotine, and alcohol close to your bedtime\\nGet regular physical activity during the daytime, at least 5 to 6 hours before going to bed.\\nAvoid naps, especially in the afternoon.\\nConsultance Number: XYZE\")\n",
    "            t += 1\n",
    "\n",
    "# Create a themed Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Conclusion Determination\")\n",
    "window.geometry(\"400x300\")  # Set initial window size\n",
    "\n",
    "# Style the GUI\n",
    "style = ttk.Style()\n",
    "style.configure(\"TLabel\", font=(\"Helvetica\", 12))\n",
    "style.configure(\"TButton\", font=(\"Helvetica\", 10))\n",
    "\n",
    "# Button to trigger the conclusion update\n",
    "update_button = ttk.Button(window, text=\"Update Conclusion\", command=update_conclusion)\n",
    "update_button.pack(pady=20)\n",
    "\n",
    "# Label to display the conclusion\n",
    "conclusion_label = ttk.Label(window, text=\"\", font=(\"Helvetica\", 14))\n",
    "conclusion_label.pack()\n",
    "\n",
    "# Run the Tkinter main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfdcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
